[ollama]
    # Ollama API base URL (use 127.0.0.1 instead of localhost for Java compatibility)
    baseUrl = "http://127.0.0.1:11434"

    # Model to use (download with: ollama pull mistral:7b)
    model = "mistral:7b"

    # Maximum tokens for response
    maxTokens = 1000

    # Temperature (0.0-2.0, lower is more deterministic)
    temperature = 0.7

[behavior]
    # Ticks between action checks (20 ticks = 1 second)
    actionTickDelay = 20

    # Allow Craftos to respond in chat
    enableChatResponses = true

    # Maximum number of Craftos that can be active simultaneously
    maxActiveCraftos = 10
