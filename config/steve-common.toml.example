[ai]
    # AI provider: 'local' (Ollama), 'groq', 'openai', or 'gemini'
    provider = "local"

[ollama]
    # Ollama API base URL (use 127.0.0.1 instead of localhost for Java compatibility)
    baseUrl = "http://127.0.0.1:11434"

    # Model to use (download with: ollama pull mistral:7b)
    model = "mistral:7b"

    # Maximum tokens for response
    maxTokens = 1000

    # Temperature (0.0-2.0, lower is more deterministic)
    temperature = 0.7

[openai]
    # Your OpenAI API key
    # Get your API key from: https://platform.openai.com/api-keys
    apiKey = "your-openai-api-key-here"

    # Using GPT-3.5-turbo (much cheaper than GPT-4 for testing)
    model = "gpt-3.5-turbo"

    # Maximum tokens per API request
    maxTokens = 1000

    # Temperature (0.0-2.0, lower is more deterministic)
    temperature = 0.7

[behavior]
    # Ticks between action checks (20 ticks = 1 second)
    actionTickDelay = 20

    # Allow Steves to respond in chat
    enableChatResponses = true

    # Maximum number of Steves that can be active simultaneously
    maxActiveSteves = 10
